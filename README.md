# Data-Lake-Spark
• Built an ETL pipeline for a data lake.
• Data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in the app.
• Loaded the data from Amazon S3.
• Processed the data into analytics tables using Apache Spark.
• Loaded the output tables into data lake (S3).
Tools used: Amazon S3, Python, Amazon EMR, Apache Spark
